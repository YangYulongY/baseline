{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# algorithms\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# optimization\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# performance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "\n",
    "# random seed \n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/yangyulong/Documents/yzkj/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_user = LabelEncoder()\n",
    "le_categ = LabelEncoder()\n",
    "\n",
    "oh_user = OneHotEncoder()\n",
    "oh_categ = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to Test Data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Owner of the challenge dataset did not release the true labels of their entire test set, only the subset that was used to calculate the public leaderboard score. I use this subset to calculate my public leaderboard score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all test data\n",
    "all_test = pd.read_pickle(data_dir + 'all_testing_aggregation.pickle')\n",
    "\n",
    "# public leaderboard subset\n",
    "public_test_labels = pd.read_csv(data_dir + 'public_labels.csv')\n",
    "public_test = all_test[all_test['session'].isin(public_test_labels['filename'])]\n",
    "\n",
    "public_test['categ_le'] = le_categ.fit_transform(public_test['categ_agg'])\n",
    "vec_size = public_test['categ_agg'].nunique()\n",
    "public_test[['oh_categ{}'.format(i) for i in range(vec_size)]] = \\\n",
    "        pd.DataFrame(oh_categ.fit_transform(\\\n",
    "        public_test['categ_le'].values.reshape(len(public_test['categ_le']), 1)).todense(), index=public_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission requirement for the challenge was that each test mouse session has an anomaly score between 0 and 1 that tells how unlikely the remote session was carried out by the respective user account, i.e., a measure of `is_illegal`=1. My classification model gives a predicted probability of `is_illegal`=1 for each mouse action in a given session; then the anomaly score of the session is the mean of the predicted probability of all its actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_proba = dict()\n",
    "\n",
    "threshold = 0.82\n",
    "\n",
    "for session in public_test['session'].unique():\n",
    "    user_test = public_test.loc[public_test['session'] == session, 'user'].unique()[0]\n",
    "    data_test = public_test[(public_test['session'] == session)]\\\n",
    "                .drop(['categ_agg', 'session', 'categ_le', 'user'], axis=1)\n",
    "\n",
    "    # load model\n",
    "    model_filename = f'clf_lgb_{user_test}.joblib'\n",
    "    model = load(model_filename)\n",
    "    \n",
    "    # apply model\n",
    "    proba = model.predict_proba(data_test)[:, 1]\n",
    "\n",
    "    session_mean_proba = np.mean(proba)\n",
    "    session_proba[session] = np.mean(proba)\n",
    "\n",
    "    if session_mean_proba > threshold:\n",
    "        print(f\"Warning: session {session}ï¼Œprob {session_mean_proba}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate final public score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(session_proba, orient='index', columns=['pred_proba'])\n",
    "public_test_labels.set_index('filename', inplace=True)\n",
    "compare_to_label = public_test_labels.join(results, sort=False)\n",
    "print('Final ROC AUC (public score): {0:0.4}'.format(roc_auc_score(compare_to_label['is_illegal'], compare_to_label['pred_proba'], average='macro')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
